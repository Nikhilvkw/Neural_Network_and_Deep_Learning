# ==== Banknote Authentication (UCI) — ANN + Graphs + Accuracy ====

# STEP 1: Imports
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras import layers, models

# STEP 2: Load Dataset
# Make sure 'banknote_authentication.csv' is in the same folder as this script / notebook
# UCI Banknote Authentication dataset columns:
# variance, skewness, curtosis, entropy, class
data_path = "banknote_authentication.csv"
df = pd.read_csv(data_path)

print("First 5 rows of dataset:")
print(df.head())
print("\nDataset Info:")
print(df.info())
print("\nClass distribution:")
print(df['class'].value_counts())

# STEP 3: Split into Features (X) and Target (y)
X = df.drop('class', axis=1).values   # all columns except 'class'
y = df['class'].values                # target: 0 = genuine, 1 = forged (depends on dataset)

# STEP 4: Train–Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("\nTrain shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

# STEP 5: Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# STEP 6: Build ANN Model
model = models.Sequential()
# Input layer + Hidden Layer 1
model.add(layers.Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)))
# Hidden Layer 2
model.add(layers.Dense(8, activation='relu'))
# Output Layer (binary classification -> 1 neuron + sigmoid)
model.add(layers.Dense(1, activation='sigmoid'))

# STEP 7: Compile Model
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

# STEP 8: Train Model
history = model.fit(
    X_train_scaled,
    y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# STEP 9: Evaluate on Test Data
test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# STEP 10: Predictions & Metrics
y_pred_prob = model.predict(X_test_scaled)
y_pred = (y_pred_prob >= 0.5).astype(int).ravel()

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nOverall Accuracy (from sklearn):", accuracy_score(y_test, y_pred))

# STEP 11: Plot Training Curves
# Accuracy Plot
plt.figure()
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Loss Plot
plt.figure()
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()
